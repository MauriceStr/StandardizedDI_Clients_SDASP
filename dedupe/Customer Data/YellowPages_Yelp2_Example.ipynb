{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'internal_path_to_file'\n",
    "input_file = 'yellow_yelp_all_pypostal.csv'\n",
    "output_file = 'yellow_yelp_all_pypostal_output1.csv'\n",
    "settings_file = 'yellow_yelp_all_pypostal_learned_settings1'\n",
    "training_file = 'yellow_yelp_all_pypostal_training1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join(folder, input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_file = os.path.join(folder, 'yellow_yelp_label2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_level = logging.INFO\n",
    "log_level = logging.DEBUG\n",
    "logging.getLogger().setLevel(log_level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv(fp, sep=',', quotechar='\"', dtype={'postalcode':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_postalcode(x):\n",
    "    \n",
    "    if x is not None:\n",
    "        subparts = str(x).split('.')\n",
    "        return subparts[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df['postalcode'] = input_df['postalcode'].apply(lambda x: get_clean_postalcode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.to_csv(fp, sep=',', quotechar='\"', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(input_df['category'].unique())\n",
    "categories = [x for x in categories if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category_corpus = input_df[['name', 'category']].drop_duplicates().to_dict(orient='records')\n",
    "category_corpus = input_df.drop_duplicates().to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(category_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_corpus = input_df[['name', 'phone']].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dedupe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import logging\n",
    "import optparse\n",
    "\n",
    "import dedupe\n",
    "from unidecode import unidecode\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(val):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        val = re.sub('  +', ' ', val)\n",
    "        val = re.sub('\\n', ' ', val)\n",
    "        val = val.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "        # If data is missing, indicate that by setting the value to `None`\n",
    "        if not val:\n",
    "            val = None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data_dict(file_path):\n",
    "    data_d = {}\n",
    "    with open(fp) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            clean_row = [(k, pre_process(v)) for (k, v) in row.items()]\n",
    "            row_id = int(row['id'])\n",
    "            data_d[row_id] = dict(clean_row)\n",
    "\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data in needed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_clean_data_dict(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Fields for dedupe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    {'field' : 'name', 'type': 'Name'},\n",
    "    {'field' : 'category', \n",
    "     'type': 'FuzzyCategorical',\n",
    "     'categories': categories,\n",
    "     'corpus': category_corpus,\n",
    "     'has missing' : True},\n",
    "    {'field' : 'name', 'type': 'String'},\n",
    "    {'field': 'postalcode', 'variable name': 'postalcode', 'type': 'Exact'},\n",
    "    {'field' : 'address', 'type': 'Address'},\n",
    "    {'field' : 'city', 'type': 'ShortString'},\n",
    "    {'field' : 'phone', 'type': 'String'},\n",
    "    {'field' : 'street', 'type': 'String', 'has missing' : True},\n",
    "    {'field' : 'house_number', 'type': 'Exists', 'has missing' : True},\n",
    "    {'field' : 'house', 'type': 'String', 'has missing' : True},\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper = dedupe.Dedupe(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.prepare_training(data_dict, blocked_proportion=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dedupe.consoleLabel(deduper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_file, 'w') as tf:\n",
    "    deduper.writeTraining(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(settings_file, 'wb') as sf:\n",
    "    deduper.writeSettings(sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run dedupe based on prior settings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper = None\n",
    "with open(settings_file, 'rb') as f:\n",
    "    deduper = dedupe.StaticDedupe(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = deduper.threshold(data_dict, recall_weight=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_dupes = deduper.match(data_dict, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# duplicate sets', len(clustered_dupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_membership = {}\n",
    "cluster_id = 0\n",
    "for (cluster_id, cluster) in enumerate(clustered_dupes):\n",
    "    id_set, scores = cluster\n",
    "    cluster_d = [data_dict[c] for c in id_set]\n",
    "    canonical_rep = dedupe.canonicalize(cluster_d)\n",
    "    for record_id, score in zip(id_set, scores):\n",
    "        cluster_membership[record_id] = {\n",
    "            \"cluster id\" : cluster_id,\n",
    "            \"canonical representation\" : canonical_rep,\n",
    "            \"confidence\": score\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleton_id = cluster_id + 1\n",
    "with open(output_file, 'w') as f_output, open(fp) as f_input:\n",
    "    writer = csv.writer(f_output)\n",
    "    reader = csv.reader(f_input)\n",
    "\n",
    "    heading_row = next(reader)\n",
    "    heading_row.insert(0, 'confidence_score')\n",
    "    heading_row.insert(0, 'Cluster ID')\n",
    "    canonical_keys = canonical_rep.keys()\n",
    "    for key in canonical_keys:\n",
    "        heading_row.append('canonical_' + key)\n",
    "\n",
    "    writer.writerow(heading_row)\n",
    "\n",
    "    for row in reader:\n",
    "        row_id = int(row[0])\n",
    "        if row_id in cluster_membership:\n",
    "            cluster_id = cluster_membership[row_id][\"cluster id\"]\n",
    "            canonical_rep = cluster_membership[row_id][\"canonical representation\"]\n",
    "            row.insert(0, cluster_membership[row_id]['confidence'])\n",
    "            row.insert(0, cluster_id)\n",
    "            for key in canonical_keys:\n",
    "                row.append(canonical_rep[key].encode('utf8'))\n",
    "        else:\n",
    "            row.insert(0, None)\n",
    "            row.insert(0, singleton_id)\n",
    "            singleton_id += 1\n",
    "            for key in canonical_keys:\n",
    "                row.append(None)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file)\n",
    "df.sort_values(['Cluster ID'], inplace=True)\n",
    "relevant_data = df[['Cluster ID', 'confidence_score', 'source', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file)\n",
    "df.sort_values(['Cluster ID'], inplace=True)\n",
    "relevant_data = df[['Cluster ID', 'confidence_score', 'source', 'id']]\n",
    "\n",
    "predictions = []\n",
    "cluster_ids = relevant_data['Cluster ID'].value_counts()\n",
    "for cluster_id in cluster_ids[cluster_ids>1].index:\n",
    "    \n",
    "    fodors_ids = relevant_data[\n",
    "        (relevant_data['Cluster ID'] == cluster_id) & \n",
    "        (relevant_data['source'] == 'yellow_pages')\n",
    "    ].id.values\n",
    "    zagats_ids = relevant_data[\n",
    "        (relevant_data['Cluster ID'] == cluster_id) & \n",
    "        (relevant_data['source'] == 'yelp')\n",
    "    ].id.values\n",
    "    \n",
    "    match_interim = list(product(fodors_ids, zagats_ids))\n",
    "    predictions.append(match_interim)\n",
    "    \n",
    "m = []\n",
    "for cluster in predictions: \n",
    "    for combo in cluster: \n",
    "        m.append([combo[0], combo[1]])\n",
    "        \n",
    "predictions = pd.DataFrame(m, columns=['yellow_pages_id', 'yelp_id'])\n",
    "\n",
    "predictions['yp-y'] = predictions.apply(lambda row: f\"{row['yellow_pages_id']}-{row['yelp_id']}\", axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.read_csv(matches_file)\n",
    "results['yp-y'] = results.apply(lambda row: f\"{row['yellow_pages_id']}-{row['yelp_id']}\", axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_duplicates = results[results['duplicate'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_dup_set = set(non_duplicates['yp-y'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = (pred_set & non_dup_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(duplicates)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_pages_dup_ids = set(duplicates['yellow_pages_id'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_dup_ids = set(duplicates['yelp_id'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_entries(x):\n",
    "    use_entry = False\n",
    "    \n",
    "    if x['yellow_pages_id'] in yellow_pages_dup_ids:\n",
    "        use_entry = True\n",
    "    elif  x['yelp_id'] in yelp_dup_ids:\n",
    "        use_entry = True\n",
    "    \n",
    "    if use_entry:\n",
    "        return None\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_comparable_with_duplicates = predictions[(predictions['yellow_pages_id'].isin(yellow_pages_dup_ids) == True) \n",
    "   | (predictions['yelp_id'].isin(yelp_dup_ids) == True) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds_comparable_with_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_set = set(preds_comparable_with_duplicates['yp-y'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = results[results['duplicate'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results[results['duplicate'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_set = set(duplicates['yp-y'].values.tolist())\n",
    "#pred_set = set(predictions['yp-y'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = len(res_set & pred_set)\n",
    "fn = len(res_set-pred_set)\n",
    "fp = len(pred_set-res_set)\n",
    "\n",
    "print(f'tp: {tp} fp: {fp} fn: {fn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r'/Hadoco/1_Standard Data Integration/Sample Datasets/Unprocessed Data/customer_samples/fodors_zagats_restaurants'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[predictions['zagats_id']==220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results[results['zagats_id']==220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Cluster ID']==0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Id'].isin(['534', '219', '221'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file)\n",
    "df.sort_values(['Cluster ID'], inplace=True)\n",
    "relevant_data = df[['Cluster ID', 'confidence_score', 'source', 'Id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "relevant_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevant_data['Cluster ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevant_data['Cluster ID'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "cluster_ids = relevant_data['Cluster ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_matches_file(matches_filepath, source1_name, source2_name):\n",
    "    results = pd.read_csv('matches_fodors_zagats.csv')\n",
    "    \n",
    "    source1_col_name = source1_name + '_id'\n",
    "    source2_col_name = source2_name + '_id'\n",
    "    combo_col_name = source1_name[0] + '-' + source2_name[0] \n",
    "    \n",
    "    results[combo_col_name] = results.apply(lambda row: \n",
    "                                   f\"{row[source1_col_name]}-{row[source2_col_name]}\", axis=1)\n",
    "    return results\n",
    "\n",
    "def prepare_predictions_file(output_file_path, source1_name, source2_name):\n",
    "    df = pd.read_csv(output_file)\n",
    "    df.sort_values(['Cluster ID'], inplace=True)\n",
    "    relevant_data = df[['Cluster ID', 'confidence_score', 'source', 'Id']]\n",
    "   \n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    cluster_ids = relevant_data['Cluster ID'].value_counts()\n",
    "    for cluster_id in cluster_ids[cluster_ids>1].index:\n",
    "\n",
    "        source1_ids = relevant_data[\n",
    "            (relevant_data['Cluster ID'] == cluster_id) & \n",
    "            (relevant_data['source'] == source1_name)\n",
    "        ].Id.values\n",
    "        source2_ids = relevant_data[\n",
    "            (relevant_data['Cluster ID'] == cluster_id) & \n",
    "            (relevant_data['source'] == source2_name)\n",
    "        ].Id.values\n",
    "\n",
    "        match_interim = list(product(source1_ids, source2_ids))\n",
    "        predictions.append(match_interim)\n",
    "\n",
    "    m = []\n",
    "    for cluster in predictions: \n",
    "        for combo in cluster: \n",
    "            m.append([combo[0], combo[1]])\n",
    "     \n",
    "    print()\n",
    "    source1_col_name = source1_name + '_id'\n",
    "    source2_col_name = source2_name + '_id'\n",
    "    \n",
    "    combo_col_name = source1_name[0] + '-' + source2_name[0] \n",
    "    \n",
    "    predictions = pd.DataFrame(m, columns=[source1_col_name, source2_col_name])\n",
    "\n",
    "    predictions[combo_col_name] = predictions.apply(lambda row: \n",
    "                                                    f\"{row[source1_col_name]}-{row[source2_col_name]}\", axis=1)\n",
    "    return predictions\n",
    "\n",
    "def calculate_f1_stats(match_set, prediction_set, mismatch_set=False):\n",
    "    calculate_f1_score = False\n",
    "    tn = None\n",
    "    precision = None\n",
    "    recall = None\n",
    "    f1 = None\n",
    "    \n",
    "    if mismatch_set:\n",
    "        calculate_f1_score = True\n",
    "        tn = len(mismatch_set - pred_set)\n",
    "\n",
    "    \n",
    "    tp = len(match_set & pred_set)\n",
    "    fn = len(match_set-pred_set)\n",
    "    fp = len(pred_set-res_set)\n",
    "    \n",
    "    if tp > 0:\n",
    "        precision = tp/(tp + fp)\n",
    "    \n",
    "    if calculate_f1_score:    \n",
    "        recall = tp/(tp + fn)\n",
    "        f1 = 2 * ((precision*recall)/(precision+recall)) \n",
    "    \n",
    "    stats = {\n",
    "    'f1_score': f1,\n",
    "    'recall': recall,\n",
    "    'precision': precision,\n",
    "    'true_positive': tp,\n",
    "    'true_negative': tn,\n",
    "    'false_positive': fp,\n",
    "    'false_negative': fn,\n",
    "    }\n",
    "        \n",
    "    return stats    \n",
    "\n",
    "\n",
    "def get_f1_stats(predictions_file_path, matches_file_path,\n",
    "                       source1_name, source2_name):\n",
    "    \n",
    "    predictions = prepare_predictions_file(\n",
    "        output_file_path = predictions_file_path,\n",
    "        source1_name = source1_name,\n",
    "        source2_name = source2_name,\n",
    "    )\n",
    "    matches = prepare_matches_file(\n",
    "        matches_filepath = matches_file_path,\n",
    "        source1_name = source1_name,\n",
    "        source2_name = source2_name,\n",
    "    )\n",
    "    \n",
    "    match_set = set(matches['f-z'].values.tolist())\n",
    "    predictions_set = set(predictions['f-z'].values.tolist())\n",
    "    \n",
    "    \n",
    "    stats = calculate_f1_stats(\n",
    "        match_set = match_set,\n",
    "        prediction_set = predictions_set\n",
    "    )\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_f1_stats(\n",
    "    predictions_file_path = output_file,\n",
    "    matches_file_path = matches_filepath,\n",
    "    source1_name = 'fodors',\n",
    "    source2_name = 'zagats'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
